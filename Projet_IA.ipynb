{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_IA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDprXmwV7AsD"
      },
      "source": [
        "# Preparing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y5XAQ9X7Tyg"
      },
      "source": [
        "IMAGE_SIZE = [256, 256]\r\n",
        "\r\n",
        "def decode(img):\r\n",
        "    image = tf.image.decode_jpeg(img, channels=3)\r\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\r\n",
        "    return image\r\n",
        "\r\n",
        "def normalize(img):\r\n",
        "    return (tf.cast(img, tf.float32) / 127.5) - 1\r\n",
        "\r\n",
        "def flip(img):\r\n",
        "    return tf.image.flip_left_right(img)\r\n",
        "\r\n",
        "def random_crop(img):\r\n",
        "    cropped_image = tf.image.random_crop(img, size=[256, 256, 3])\r\n",
        "    return cropped_image\r\n",
        "\r\n",
        "def random_jitter(img):\r\n",
        "    image = tf.image.resize(img, [int(256*1.3), int(256*1.3)],\r\n",
        "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\r\n",
        "    image = random_crop(image)\r\n",
        "    return image\r\n",
        "\r\n",
        "def preprocess_image_train(img, label=None):\r\n",
        "    image = random_jitter(img)\r\n",
        "    return image\r\n",
        "\r\n",
        "def read_tfrecord(example):\r\n",
        "    tfrecord_format = {\r\n",
        "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\r\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),\r\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.string)\r\n",
        "    }\r\n",
        "    example = tf.io.parse_single_example(example, tfrecord_format)\r\n",
        "    image = decode(example['image'])\r\n",
        "    return image\r\n",
        "\r\n",
        "def load_dataset(filenames, labeled=False, ordered=False, repeats=200):\r\n",
        "    dataset = tf.data.TFRecordDataset(filenames)\r\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\r\n",
        "    dataset = dataset.concatenate(dataset.map(flip, num_parallel_calls=AUTOTUNE).shuffle(100000))\r\n",
        "    dataset = dataset.concatenate(dataset.map(random_jitter, num_parallel_calls=AUTOTUNE).shuffle(10000, reshuffle_each_iteration=True).repeat(repeats))\r\n",
        "    dataset = dataset.map(normalize, num_parallel_calls=AUTOTUNE).shuffle(10000)\r\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioVAuckN7ho4"
      },
      "source": [
        "monet_ds = load_dataset(MONET_FILENAMES, labeled=True, repeats=50).batch(100, drop_remainder=True)\r\n",
        "photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True, repeats=2  ).batch(100, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMsmy5sg53wa"
      },
      "source": [
        "# Upsamplin and Downsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQkl4XawsdBZ"
      },
      "source": [
        "OUTPUT_CHANNELS = 3\r\n",
        "LATENT_DIM = 1024\r\n",
        "\r\n",
        "def downsampling(filters, size, apply_instancenorm=True):\r\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\r\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\r\n",
        "\r\n",
        "    r = keras.Sequential()\r\n",
        "    r.add(layers.Conv2D(filters, size, padding='same',\r\n",
        "                             kernel_initializer=initializer, use_bias=False))\r\n",
        "    r.add(layers.MaxPool2D())\r\n",
        "\r\n",
        "    if apply_instancenorm:\r\n",
        "        r.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\r\n",
        "\r\n",
        "    r.add(layers.LeakyReLU())\r\n",
        "\r\n",
        "    return r\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYNd_bedtLwM"
      },
      "source": [
        "def upsampling(filters, size, apply_dropout=False):\r\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\r\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\r\n",
        "\r\n",
        "    r = keras.Sequential()\r\n",
        "    r.add(layers.Conv2DTranspose(filters, size, strides=2,\r\n",
        "                                      padding='same',\r\n",
        "                                      kernel_initializer=initializer,\r\n",
        "                                      use_bias=False))\r\n",
        "\r\n",
        "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\r\n",
        "\r\n",
        "    if apply_dropout:\r\n",
        "        r.add(layers.Dropout(0.5))\r\n",
        "\r\n",
        "    r.add(layers.LeakyReLU())\r\n",
        "\r\n",
        "    return result\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVnNlw-b6DAx"
      },
      "source": [
        "EPOCHS = 25\r\n",
        "\r\n",
        "LR_G = 2e-4\r\n",
        "LR_D = 2e-4\r\n",
        "beta_1 = .5\r\n",
        "\r\n",
        "RLabel = .9\r\n",
        "Flabel = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXZ0iVu26rH7"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bggEa8y1uZmX"
      },
      "source": [
        "def Generator():\r\n",
        "    inputs = layers.Input(shape=[256,256,3])\r\n",
        "\r\n",
        "    downstack = [\r\n",
        "        downsampling(64, 4, apply_instancenorm=False),\r\n",
        "        downsampling(128, 4), \r\n",
        "        downsampling(256, 4), \r\n",
        "        downsampling(512, 4), \r\n",
        "        downsampling(512, 4), \r\n",
        "        downsampling(512, 4), \r\n",
        "        downsampling(512, 4), \r\n",
        "        downsampling(512, 4), \r\n",
        "    ]\r\n",
        "\r\n",
        "    upstack = [\r\n",
        "        upsampling(512, 4, apply_dropout=True), \r\n",
        "        upsampling(512, 4, apply_dropout=True), \r\n",
        "        upsampling(512, 4, apply_dropout=True), \r\n",
        "        upsampling(512, 4), \r\n",
        "        upsampling(256, 4), \r\n",
        "        upsampling(128, 4), \r\n",
        "        upsampling(64, 4), \r\n",
        "    ]\r\n",
        "\r\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\r\n",
        "    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\r\n",
        "                                  strides=2,\r\n",
        "                                  padding='same',\r\n",
        "                                  kernel_initializer=initializer,\r\n",
        "                                  activation='tanh') # (bs, 256, 256, 3)\r\n",
        "\r\n",
        "    x = inputs\r\n",
        "\r\n",
        "    s = []\r\n",
        "    for d in downstack:\r\n",
        "        x = d(x)\r\n",
        "        s.append(x)\r\n",
        "\r\n",
        "    s = reversed(s[:-1])\r\n",
        "\r\n",
        "    # Upsampling and establishing the skip connections\r\n",
        "    for up, i in zip(upstack, s):\r\n",
        "        x = up(x)\r\n",
        "        x = layers.Concatenate()([x, i])\r\n",
        "\r\n",
        "    x = last(x)\r\n",
        "\r\n",
        "    return keras.Model(inputs=inputs, outputs=x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFJk_kO16x_R"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4AlDTx5v0y7"
      },
      "source": [
        "def discriminator():\r\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\r\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\r\n",
        "\r\n",
        "    inp = layers.Input(shape=[256, 256, 3], name='input_image')\r\n",
        "\r\n",
        "    x = inp\r\n",
        "\r\n",
        "    d1 = downsampling(64, 4, False)(x) \r\n",
        "    d2 = downsampling(128, 4)(d1)\r\n",
        "    d3 = downsampling(256, 4)(d2) \r\n",
        "\r\n",
        "    zero_pad1 = layers.ZeroPadding2D()(3) \r\n",
        "    conv = layers.Conv2D(512, 4, strides=1,\r\n",
        "                         kernel_initializer=initializer,\r\n",
        "                         use_bias=False)(zero_pad1) \r\n",
        "\r\n",
        "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\r\n",
        "\r\n",
        "    leaky_relu = layers.LeakyReLU()(norm1)\r\n",
        "\r\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) \r\n",
        "\r\n",
        "    last_conv = layers.Conv2D(1, 4, strides=1,\r\n",
        "                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\r\n",
        "\r\n",
        "    last_relu = layers.LeakyReLU(alpha=0.2)(last_conv)\r\n",
        "    last_pool = layers.Flatten()(last_relu)\r\n",
        "    last = layers.Dense(1, activation='sigmoid')(last_pool)\r\n",
        "\r\n",
        "    return tf.keras.Model(inputs=inp, outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP6ZmCZOwqVW"
      },
      "source": [
        "monet_CycleGenerator = generator() \r\n",
        "monet_CycleDiscriminator = discriminator()\r\n",
        "\r\n",
        "photo_CycleGenerator = generator() \r\n",
        "photo_CycleDiscriminator = discriminator() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYc7e6pKyxqF"
      },
      "source": [
        "class CycleGan(keras.Model):\r\n",
        "    def __init__(self,monetG,photoG,monetD,photoD,lambda_cycle=10,Rlabel=.5):\r\n",
        "        super(CycleGan, self).__init__()\r\n",
        "        self.m_gen = monetG\r\n",
        "        self.p_gen = photoG\r\n",
        "        self.m_disc = monetD\r\n",
        "        self.p_disc = photoD\r\n",
        "        self.lambda_cycle = lambda_cycle\r\n",
        "        self.Rlabel = Rlabel\r\n",
        "        \r\n",
        "    def compile(self,m_gen_optimizer,p_gen_optimizer,m_disc_optimizer,p_disc_optimizer,gen_loss_fn,disc_loss_fn,cycle_loss_fn,identity_loss_fn):\r\n",
        "        super(CycleGan, self).compile()\r\n",
        "        self.m_gen_optimizer = m_gen_optimizer\r\n",
        "        self.p_gen_optimizer = p_gen_optimizer\r\n",
        "        self.m_disc_optimizer = m_disc_optimizer\r\n",
        "        self.p_disc_optimizer = p_disc_optimizer\r\n",
        "        self.gen_loss_fn = gen_loss_fn\r\n",
        "        self.disc_loss_fn = disc_loss_fn\r\n",
        "        self.cycle_loss_fn = cycle_loss_fn\r\n",
        "        self.identity_loss_fn = identity_loss_fn\r\n",
        "        \r\n",
        "    def train_step(self, batch_data):\r\n",
        "        realMonet, realPhoto = batch_data\r\n",
        "        \r\n",
        "        batch_size = tf.shape(realPphoto)[0]\r\n",
        "        labels_real = tf.zeros((batch_size, 1)) + self.Rlabel\r\n",
        "        labels_real += 0.05 * tf.random.uniform(tf.shape(labels_real))        \r\n",
        "        \r\n",
        "        with tf.GradientTape(persistent=True) as tape:\r\n",
        "            fakeMonet = self.m_gen(reaPhoto, training=True)\r\n",
        "            cycled_photo = self.p_gen(fakeMonet, training=True)\r\n",
        "\r\n",
        "            \r\n",
        "            fakePhoto = self.p_gen(realMonet, training=True)\r\n",
        "            cycledMonet = self.m_gen(fakePhoto, training=True)\r\n",
        "\r\n",
        "            \r\n",
        "            same_monet = self.m_gen(realMonet, training=True)\r\n",
        "            same_photo = self.p_gen(realPhoto, training=True)\r\n",
        "\r\n",
        "            \r\n",
        "            disc_real_monet = self.m_disc(realMonet, training=True)\r\n",
        "            disc_real_photo = self.p_disc(realPhoto, training=True)\r\n",
        "\r\n",
        "            \r\n",
        "            disc_fake_monet = self.m_disc(fakeMonet, training=True)\r\n",
        "            disc_fake_photo = self.p_disc(fakePhoto, training=True)\r\n",
        "\r\n",
        "            \r\n",
        "            monet_gen_loss = self.gen_loss_fn(disc_real_monet, disc_fake_monet, labels_real)\r\n",
        "            photo_gen_loss = self.gen_loss_fn(disc_real_photo, disc_fake_photo, labels_real)\r\n",
        "\r\n",
        "            \r\n",
        "            total_cycle_loss = self.cycle_loss_fn(realMonet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(realPhoto, cycled_photo, self.lambda_cycle)\r\n",
        "\r\n",
        "            \r\n",
        "            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(realMonet, same_monet, self.lambda_cycle)\r\n",
        "            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(realPhoto, same_photo, self.lambda_cycle)\r\n",
        "\r\n",
        "            \r\n",
        "            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet, labels_real)\r\n",
        "            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo, labels_real)\r\n",
        "\r\n",
        "        \r\n",
        "        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\r\n",
        "                                                  self.m_gen.trainable_variables)\r\n",
        "        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\r\n",
        "                                                  self.p_gen.trainable_variables)\r\n",
        "\r\n",
        "        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\r\n",
        "                                                      self.m_disc.trainable_variables)\r\n",
        "        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\r\n",
        "                                                      self.p_disc.trainable_variables)\r\n",
        "\r\n",
        "        \r\n",
        "        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\r\n",
        "                                                 self.m_gen.trainable_variables))\r\n",
        "\r\n",
        "        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\r\n",
        "                                                 self.p_gen.trainable_variables))\r\n",
        "\r\n",
        "        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\r\n",
        "                                                  self.m_disc.trainable_variables))\r\n",
        "\r\n",
        "        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\r\n",
        "                                                  self.p_disc.trainable_variables))\r\n",
        "        \r\n",
        "        return {\r\n",
        "            \"monet_gen_loss\": total_monet_gen_loss,\r\n",
        "            \"photo_gen_loss\": total_photo_gen_loss,\r\n",
        "            \"monet_disc_loss\": monet_disc_loss,\r\n",
        "            \"photo_disc_loss\": photo_disc_loss\r\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoUdhxjg0C2G"
      },
      "source": [
        "def discriminator_loss(predictions_real, predictions_gen, labels_real):\r\n",
        "        return (tf.reduce_mean((predictions_gen  - tf.reduce_mean(predictions_real) + labels_real) ** 2) +\r\n",
        "                tf.reduce_mean((predictions_real - tf.reduce_mean(predictions_gen)  - labels_real) ** 2))/2\r\n",
        "    \r\n",
        "def generator_loss(predictions_real, predictions_gen, labels_real):\r\n",
        "        return (tf.reduce_mean((predictions_real - tf.reduce_mean(predictions_gen)  + labels_real) ** 2) +\r\n",
        "                tf.reduce_mean((predictions_gen  - tf.reduce_mean(predictions_real) - labels_real) ** 2)) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsoCtIKo0oJt"
      },
      "source": [
        "def calc_cycle_loss(real_image, cycled_image, LAMBDA):\r\n",
        "        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\r\n",
        "\r\n",
        "        return LAMBDA * loss1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5xmnXio0sQX"
      },
      "source": [
        "with x:\r\n",
        "    def identity_loss(real_image, same_image, LAMBDA):\r\n",
        "        loss = tf.reduce_mean(tf.abs(real_image - same_image))\r\n",
        "        return LAMBDA * 0.5 * loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi28wG800s2R"
      },
      "source": [
        "monet_generator_optimizer = tf.keras.optimizers.Adam(LR_G, beta_1=0.5)\r\n",
        "monet_discriminator_optimizer = tf.keras.optimizers.Adam(LR_D, beta_1=0.5)\r\n",
        "\r\n",
        "photo_generator_optimizer = tf.keras.optimizers.Adam(LR_G, beta_1=0.5)\r\n",
        "photo_discriminator_optimizer = tf.keras.optimizers.Adam(LR_D, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD3YVcfb0xfe"
      },
      "source": [
        "cycle_gan_model = CycleGan(\r\n",
        "        monet_cycleGenerator, photo_cycleGenerator, monet_cycleDiscriminator, photo_cycleDiscriminator, label=0.66\r\n",
        ")\r\n",
        "\r\n",
        "cycle_gan_model.compile(\r\n",
        "        m_gen_optimizer = monet_generator_optimizer,\r\n",
        "        p_gen_optimizer = photo_generator_optimizer,\r\n",
        "        m_disc_optimizer = monet_discriminator_optimizer,\r\n",
        "        p_disc_optimizer = photo_discriminator_optimizer,\r\n",
        "        gen_loss_fn = generator_loss,\r\n",
        "        disc_loss_fn = discriminator_loss,\r\n",
        "        cycle_loss_fn = calc_cycle_loss,\r\n",
        "        identity_loss_fn = identity_loss\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNj1eLpy04AR"
      },
      "source": [
        "cycle_gan_model.fit(\r\n",
        "    tf.data.Dataset.zip((monet_ds, photo_ds)),\r\n",
        "    epochs=EPOCHS\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9SgsRm82Bhy"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR0ntO83BcQT"
      },
      "source": [
        "def view_image(ds, rows=2):\r\n",
        "    image = next(iter(ds))\r\n",
        "    image = image.numpy()\r\n",
        "\r\n",
        "    fig = plt.figure(figsize=(22, rows * 5 ))\r\n",
        "    for i in range(5 * rows):\r\n",
        "        ax = fig.add_subplot(rows, 5, i+1, xticks=[], yticks=[])\r\n",
        "        ax.imshow(image[i] / 2 + .5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r_IVMl1073X"
      },
      "source": [
        "_, ax = plt.subplots(2, 5, figsize=(12, 6))\r\n",
        "for i, img in enumerate(photo_ds.take(5)):\r\n",
        "    generated = monet_cycleGenerator(img, training=False)[0].numpy()\r\n",
        "    generated = (generated * 127.5 + 127.5).astype(np.uint8)\r\n",
        "    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\r\n",
        "\r\n",
        "    ax[0, i].imshow(img)\r\n",
        "    ax[1, i].imshow(generated)\r\n",
        "    ax[0, i].set_title(\"Input\")\r\n",
        "    ax[1, i].set_title(\"Output\")\r\n",
        "    ax[0, i].axis(\"off\")\r\n",
        "    ax[1, i].axis(\"off\")\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}